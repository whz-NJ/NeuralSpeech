本地导出aiui的 aiui_ist_sentence 表记录，执行 CorpusExtrator.java 抽取语料
本地执行 gen_sports_corpus_txt.py 抽取龙猫语料，转换为标准化格式的 .txt 文件
本地执行 IstNoiseGenerator.java 将语料(docx和aiui_football)通过 tts/ist 处理，得到原始语料、ASR转写语料对 (要求每行输入是单句，因为IstNoiseGenerator不分句)
本地执行 modify_longmao_corpus.py 调整龙猫语料（包括 aiui_football 文件）中的数字风格保持和ASR模型一致
本地执行 wiki_sports_asr_preprocess.py 将wiki、运动语料标准化，并抽取字典 (std_sports_corpus_en 目录)，手工拷贝到 dictionary目录
本地执行 EnWordsNoiseGenerator.java 得到字典中包含的英文单词的纠错集
本地执行 short_dictionary_corrections.py 精简词表（先取字典的前4万个词，再看这4万个词讯飞误转词，一起加入词表）
# 执行五遍 align_split_noise_std_wiki.py 给wiki语料添加不同噪声，并划分训练/测试/验证集，并对齐（运动语料经过ASR转写已经带有噪声了）
本地执行五遍 split_align_noise_std_wiki.py 给wiki语料添加不同噪声，并划分训练/测试/验证集，并对齐（运动语料经过ASR转写已经带有噪声了。在测试环境 11/12/14/16/17上执行）
九天执行 ftb_asr_files_split.py 将运动语料拆分为训练集、验证集、测试集
九天执行 gen_hypo_ref_file.py 将运动语料分成 hypo* 和 ref* 文件。
九天执行 align_cal_werdur_v2.py 对齐运动语料 hypo* 和 ref* 文件
九天执行下面命令，构造二进制化需要的数据文件：
    cp hypo_train_std_noised_corpus.txt.src.werdur.full train.zh_CN
    cp ref_train_std_noised_corpus.txt.tgt train.zh_CN_tgt
    cp hypo_valid_std_noised_corpus.txt.src.werdur.full valid.zh_CN
    cp ref_valid_std_noised_corpus.txt.tgt valid.zh_CN_tgt
    cp hypo_test_std_noised_corpus.txt.src.werdur.full test.zh_CN
    cp ref_test_std_noised_corpus.txt.tgt test.zh_CN_tgt

	cat hypo_train_std_noised9_corpus.full >> train.zh_CN
	cat hypo_train_std_noised7_corpus.full >> train.zh_CN
	cat hypo_train_std_noised5_corpus.full >> train.zh_CN
	cat hypo_train_std_noised3_corpus.full >> train.zh_CN
	cat hypo_train_std_noised1_corpus.full >> train.zh_CN

	cat ref_train_std_noised9_corpus.tgt >> train.zh_CN_tgt
	cat ref_train_std_noised7_corpus.tgt >> train.zh_CN_tgt
	cat ref_train_std_noised5_corpus.tgt >> train.zh_CN_tgt
	cat ref_train_std_noised3_corpus.tgt >> train.zh_CN_tgt
	cat ref_train_std_noised1_corpus.tgt >> train.zh_CN_tgt


	cat hypo_valid_std_noised9_corpus.full >> valid.zh_CN
	cat hypo_valid_std_noised7_corpus.full >> valid.zh_CN
	cat hypo_valid_std_noised5_corpus.full >> valid.zh_CN
	cat hypo_valid_std_noised3_corpus.full >> valid.zh_CN
	cat hypo_valid_std_noised1_corpus.full >> valid.zh_CN

	cat ref_valid_std_noised9_corpus.tgt >> valid.zh_CN_tgt
	cat ref_valid_std_noised7_corpus.tgt >> valid.zh_CN_tgt
	cat ref_valid_std_noised5_corpus.tgt >> valid.zh_CN_tgt
	cat ref_valid_std_noised3_corpus.tgt >> valid.zh_CN_tgt
	cat ref_valid_std_noised1_corpus.tgt >> valid.zh_CN_tgt


	cat hypo_test_std_noised9_corpus.full >> test.zh_CN
	cat hypo_test_std_noised7_corpus.full >> test.zh_CN
	cat hypo_test_std_noised5_corpus.full >> test.zh_CN
	cat hypo_test_std_noised3_corpus.full >> test.zh_CN
	cat hypo_test_std_noised1_corpus.full >> test.zh_CN

	cat ref_test_std_noised9_corpus.tgt >> test.zh_CN_tgt
	cat ref_test_std_noised7_corpus.tgt >> test.zh_CN_tgt
	cat ref_test_std_noised5_corpus.tgt >> test.zh_CN_tgt
	cat ref_test_std_noised3_corpus.tgt >> test.zh_CN_tgt
	cat ref_test_std_noised1_corpus.tgt >> test.zh_CN_tgt
九天执行 data-gen.sh 将对齐结果二进制化
九天执行 train_pretrain.sh 训练模型
九天执行 train_ft.sh 对模型微调
九天执行 gen_asr_eval_data.py 生成测试集的 data.json
九天执行 test_ft.sh 得到 /root/fastcorrect/asr_eval_data 目录test或dev子目录下数据的模型预测效果（即生成 data.json）
九天执行 cal_wer_asr.sh 展示模型在验证集上的效果（因为输入是个data.json文件路径，这个路径可以是原始ASR输出或模型输出，结果存储在 wer_short.txt ）
部署到阿里云，本地执行 call_fc_gen_json.py 脚本，验证未学习语料效果。


千万不要犯把龙猫语料做fc纠错，再和龙猫比的错误。
应该用讯飞转写再fc纠错，再和原始龙猫比较。

输入是一个汉字的 one-hot encoding 编码，embedding 层是 512 维
encoder 是8个头，6层，中间层特征向量编码 512 维
decoder 是8个头，6层，中间层特征向量编码 512 维
DurationPredictor 5层，每层是1维卷积+relu激活函数+层归一化+Dropout，最后是 线性层+relu+层归一化+Dropout

计算ngram-score，如果达不到平均数，则加入疑似错误
找到疑似错误的同音字，计算替换后的ppl-score，如果增加了，则替换
kenlm模型需要语料训练，准确率有限。
